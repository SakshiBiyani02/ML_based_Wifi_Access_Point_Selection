# -*- coding: utf-8 -*-
"""vsvsvs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q3obcTaWQfoBfqN9Brs_aL9j26TILxBE
"""

# ==================================================
# Imports
# ==================================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import (
    train_test_split, StratifiedKFold, cross_val_score, GridSearchCV
)
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    classification_report, accuracy_score, confusion_matrix,
    top_k_accuracy_score, log_loss, matthews_corrcoef, roc_auc_score
)
import joblib

# ==================================================
# 1. Load Dataset
# ==================================================
df = pd.read_csv("updated_project_dataset.csv")
print("Dataset shape:", df.shape)

# ==================================================
# 2. Drop Non-Numeric Columns
# ==================================================
non_numeric_cols = ['BSSID', 'recordDate', 'ssid', 'securityType', 'capabilities']
df = df.drop(columns=non_numeric_cols, errors='ignore')
print("Dropped columns:", non_numeric_cols)

# ==================================================
# 3. Define Features & Target
# ==================================================
X = df.drop(columns=['Best_AP_Label'])
y = df['Best_AP_Label']
print("Initial AP classes:", y.nunique())

# ==================================================
# 4. Filter Classes with < 2 Samples
# ==================================================
counts = y.value_counts()
valid_classes = counts[counts > 1].index
df = df[df['Best_AP_Label'].isin(valid_classes)]

X = df.drop(columns=['Best_AP_Label'])
y = df['Best_AP_Label']
print("After filtering shape:", df.shape)
print("Remaining AP classes:", y.nunique())

# Encode target labels
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# ==================================================
# 5. Train-Test Split
# ==================================================
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded,
    test_size=0.2,
    random_state=42,
    stratify=y_encoded
)

# ==================================================
# 6. Hyperparameter Tuning (GridSearchCV)
# ==================================================
param_grid = {
    'n_estimators': [100, 300, 500],
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5]
}

rf_base = RandomForestClassifier(random_state=42, n_jobs=-1)

grid_search = GridSearchCV(
    estimator=rf_base,
    param_grid=param_grid,
    scoring='accuracy',
    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
    n_jobs=-1,
    verbose=1
)

print("\n==== Starting Hyperparameter Tuning... ====")
grid_search.fit(X_train, y_train)

rf_best = grid_search.best_estimator_
print("\n==== Best Hyperparameters Found ====")
print(grid_search.best_params_)
print("Best CV Score:", grid_search.best_score_)

# ==================================================
# 7. Cross-Validation with Best Model
# ==================================================
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = cross_val_score(rf_best, X, y_encoded, cv=cv, scoring="accuracy", n_jobs=-1)

print("\n==== Cross-Validation Results (5-fold) ====")
print("Scores:", cv_scores)
print("Mean CV Accuracy:", np.mean(cv_scores))

# ==================================================
# 8. Train & Evaluate Best Model
# ==================================================
# Train the best model on the full training set
rf_best.fit(X_train, y_train)

# Make predictions and get probabilities
y_train_pred = rf_best.predict(X_train)
y_test_pred = rf_best.predict(X_test)
y_test_proba = rf_best.predict_proba(X_test)

# --- Training report
print("\n==== Training Metrics ====")
print("Accuracy:", accuracy_score(y_train, y_train_pred))
print(classification_report(
    y_train, y_train_pred,
    labels=np.unique(y_train),
    target_names=le.classes_[np.unique(y_train)]
))

# --- Testing report
print("\n==== Testing Metrics ====")
print("Top-1 Accuracy:", accuracy_score(y_test, y_test_pred))

top5_acc = top_k_accuracy_score(
    y_test, y_test_proba, k=5,
    labels=range(len(le.classes_))
)
print("Top-5 Accuracy:", top5_acc)

# Additional validation metrics
logloss = log_loss(y_test, y_test_proba, labels=range(len(le.classes_)))
print(f"Log-Loss: {logloss:.4f}")

mcc = matthews_corrcoef(y_test, y_test_pred)
print(f"Matthews Correlation Coefficient (MCC): {mcc:.4f}")


# Final classification report on the test set
print(classification_report(
    y_test, y_test_pred,
    labels=np.unique(y_test),
    target_names=le.classes_[np.unique(y_test)],
    zero_division=0
))

# ==================================================
# 9. Confusion Matrix Heatmap
# ==================================================
cm = confusion_matrix(y_test, y_test_pred, labels=np.unique(y_test))
plt.figure(figsize=(18, 14))
sns.heatmap(cm, annot=False, cmap="Blues",
             xticklabels=le.classes_[np.unique(y_test)],
             yticklabels=le.classes_[np.unique(y_test)])
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix Heatmap (Test Set) - Optimized Model")
plt.tight_layout()
plt.savefig('optimized_confusion_matrix_heatmap.png')
plt.clf()

# ==================================================
# 10. Feature Importance Plot
# ==================================================
feature_importances = pd.Series(rf_best.feature_importances_, index=X_train.columns)
sorted_features = feature_importances.sort_values(ascending=False)

plt.figure(figsize=(12, 8))
sns.barplot(x=sorted_features.values, y=sorted_features.index, palette='viridis')
plt.title('Feature Importances (Optimized Model)')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.tight_layout()
plt.savefig('optimized_feature_importance.png')
plt.clf()

# ==================================================
# 11. Correlation Matrix Heatmap
# ==================================================
correlation_matrix = X.corr()
plt.figure(figsize=(18, 16))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm')
plt.title('Correlation Matrix of All Features')
plt.tight_layout()
plt.savefig('correlation_matrix_heatmap.png')
plt.clf()

# ==================================================
# 12. Save the trained model for future use
# ==================================================
joblib.dump(rf_best, 'optimized_model.pkl')
joblib.dump(le, 'label_encoder.pkl')
print("\nOptimized model and label encoder saved to disk.")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier

# ===========================
# 1. Load the dataset
# ===========================
df = pd.read_csv("updated_project_dataset.csv")

# ===========================
# 2. Preprocess the data (as in your original code)
# ===========================
non_numeric_cols = ['BSSID', 'recordDate', 'ssid', 'securityType', 'capabilities']
df = df.drop(columns=non_numeric_cols, errors='ignore')

# Features and Target
X = df.drop(columns=['Best_AP_Label'])
y = df['Best_AP_Label']

# Remove classes with < 2 samples
counts = y.value_counts()
valid_classes = counts[counts > 1].index
df = df[df['Best_AP_Label'].isin(valid_classes)]

X = df.drop(columns=['Best_AP_Label'])
y = df['Best_AP_Label']

# Encode target
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# Train-test split (required for model training)
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
)

# ===========================
# 3. Train the Random Forest model
# ===========================
rf = RandomForestClassifier(
    n_estimators=300,
    max_depth=20,
    min_samples_split=5,
    random_state=42,
    n_jobs=-1
)
rf.fit(X_train, y_train)

# ===========================
# 4. Define five different live samples
# ===========================
# Each dictionary represents one live data sample
live_samples = [
    {
        'latitude': 52.2530, 'longitude': -2.8910, 'accuracy': 5.5, 'RSSI': -30,
        'Channel': 1, 'frequency': 2417, 'totalSurveys': 7500, 'inconsistentLocation': 0,
        'SNR': 60, 'Estimated_Distance': 0.75, 'AP_Load': 20, 'Congestion_Level': 150000.0,
        'Throughput': 4.5, 'Packet_Loss': 55, 'Latency': 50000.0, 'Jitter': 7.0,
        'Optimal_AP': 0, 'Optimal_Rate': 24, 'distance_moved': 15.0, 'Mobility_Flag': True,
        'Recovered_From_Disconnect': False, 'Avg_Neighbor_RSSI': -78.0,
        'Avg_Throughput_BSSID': 4.5, 'Avg_PacketLoss_BSSID': 55.0,
        'Fairness_Index': 0.45, 'Composite_Score': 0.25
    },
    {
        'latitude': 52.2515, 'longitude': -2.8890, 'accuracy': 7.0, 'RSSI': -50,
        'Channel': 6, 'frequency': 2437, 'totalSurveys': 8100, 'inconsistentLocation': 0,
        'SNR': 40, 'Estimated_Distance': 1.5, 'AP_Load': 35, 'Congestion_Level': 200000.0,
        'Throughput': 3.0, 'Packet_Loss': 70, 'Latency': 60000.0, 'Jitter': 9.0,
        'Optimal_AP': 0, 'Optimal_Rate': 18, 'distance_moved': 25.0, 'Mobility_Flag': True,
        'Recovered_From_Disconnect': False, 'Avg_Neighbor_RSSI': -85.0,
        'Avg_Throughput_BSSID': 3.0, 'Avg_PacketLoss_BSSID': 70.0,
        'Fairness_Index': 0.35, 'Composite_Score': 0.15
    },
    {
        'latitude': 52.2525, 'longitude': -2.8905, 'accuracy': 6.0, 'RSSI': -40,
        'Channel': 11, 'frequency': 2462, 'totalSurveys': 8500, 'inconsistentLocation': 0,
        'SNR': 50, 'Estimated_Distance': 1.0, 'AP_Load': 28, 'Congestion_Level': 180000.0,
        'Throughput': 4.0, 'Packet_Loss': 65, 'Latency': 55000.0, 'Jitter': 8.0,
        'Optimal_AP': 0, 'Optimal_Rate': 24, 'distance_moved': 20.0, 'Mobility_Flag': True,
        'Recovered_From_Disconnect': False, 'Avg_Neighbor_RSSI': -80.0,
        'Avg_Throughput_BSSID': 4.0, 'Avg_PacketLoss_BSSID': 65.0,
        'Fairness_Index': 0.40, 'Composite_Score': 0.20
    },
    {
        'latitude': 52.2540, 'longitude': -2.8920, 'accuracy': 8.0, 'RSSI': -60,
        'Channel': 36, 'frequency': 5180, 'totalSurveys': 9000, 'inconsistentLocation': 0,
        'SNR': 30, 'Estimated_Distance': 2.0, 'AP_Load': 40, 'Congestion_Level': 250000.0,
        'Throughput': 2.5, 'Packet_Loss': 80, 'Latency': 70000.0, 'Jitter': 10.0,
        'Optimal_AP': 0, 'Optimal_Rate': 12, 'distance_moved': 30.0, 'Mobility_Flag': False,
        'Recovered_From_Disconnect': True, 'Avg_Neighbor_RSSI': -90.0,
        'Avg_Throughput_BSSID': 2.5, 'Avg_PacketLoss_BSSID': 80.0,
        'Fairness_Index': 0.30, 'Composite_Score': 0.10
    },
    {
        'latitude': 52.2510, 'longitude': -2.8885, 'accuracy': 5.0, 'RSSI': -20,
        'Channel': 48, 'frequency': 5240, 'totalSurveys': 7800, 'inconsistentLocation': 0,
        'SNR': 70, 'Estimated_Distance': 0.5, 'AP_Load': 15, 'Congestion_Level': 120000.0,
        'Throughput': 5.5, 'Packet_Loss': 50, 'Latency': 45000.0, 'Jitter': 6.0,
        'Optimal_AP': 0, 'Optimal_Rate': 36, 'distance_moved': 10.0, 'Mobility_Flag': True,
        'Recovered_From_Disconnect': False, 'Avg_Neighbor_RSSI': -75.0,
        'Avg_Throughput_BSSID': 5.5, 'Avg_PacketLoss_BSSID': 50.0,
        'Fairness_Index': 0.50, 'Composite_Score': 0.30
    }
]

# ===========================
# 5. Predict for each live sample
# ===========================
print("==== Live Sample Predictions ====")
for i, sample in enumerate(live_samples):
    live_sample_df = pd.DataFrame([sample])

    # Predict the best AP (Top-1 prediction)
    top1_prediction = rf.predict(live_sample_df)
    best_ap_label = le.inverse_transform(top1_prediction)[0]

    # Predict probabilities for each AP
    top5_probabilities = rf.predict_proba(live_sample_df)[0]
    top5_indices = np.argsort(top5_probabilities)[::-1][:5]
    top5_ap_labels = le.inverse_transform(top5_indices)

    # Print results for the current sample
    print(f"\n--- Sample {i+1} ---")
    print(f"Predicted Best AP: {best_ap_label}")
    print(f"Top 5 Recommended APs: {top5_ap_labels}")

import joblib
import pandas as pd
import numpy as np
import pprint
import re
import math
import random

# ==================================================
# 1. Raw Text Output from 'netsh' command
# ==================================================
# Replace this with the live output you get from your terminal.
# Make sure to remove any leading command-line text like 'C:\Users\SAKSHI>'.
NETSH_OUTPUT = """
Interface name : Wi-Fi
There are 2 networks currently visible.

SSID 1 : VITC-HOS2-4
    Network type            : Infrastructure
    Authentication          : WPA2-Enterprise
    Encryption              : CCMP
    BSSID 1                 : 70:3a:0e:e8:20:a0
          Signal              : 22%
          Radio type          : 802.11n
          Band                : 2.4 GHz
          Channel             : 11
          QoS MSCS Supported    : 0
          QoS Map Supported     : 0
          Basic rates (Mbps) : 5.5 6 9 12 24
          Other rates (Mbps) : 11 18 36 48 54
    BSSID 2                 : d4:bd:4f:52:dc:20
          Signal              : 26%
          Radio type          : 802.11n
          Band                : 2.4 GHz
          Channel             : 8
          Bss Load:
              Connected Stations:        0
              Channel Utilization:       9 (3 %)
              Medium Available Capacity: 0 (0 us/s)
          QoS MSCS Supported    : 0
          QoS Map Supported     : 0
          Basic rates (Mbps) : 12 24
          Other rates (Mbps) : 18 36 48 54
    BSSID 3                 : d4:bd:4f:55:aa:20
          Signal              : 29%
          Radio type          : 802.11n
          Band                : 2.4 GHz
          Channel             : 8
          Bss Load:
              Connected Stations:        1
              Channel Utilization:       34 (13 %)
              Medium Available Capacity: 0 (0 us/s)
          QoS MSCS Supported    : 0
          QoS Map Supported     : 0
          Basic rates (Mbps) : 12 24
          Other rates (Mbps) : 18 36 48 54
    BSSID 4                 : 70:3a:0e:e8:10:20
          Signal              : 35%
          Radio type          : 802.11n
          Band                : 2.4 GHz
          Channel             : 11
          QoS MSCS Supported    : 0
          QoS Map Supported     : 0
          Basic rates (Mbps) : 5.5 6 9 12 24
          Other rates (Mbps) : 11 18 36 48 54
    BSSID 5                 : d4:bd:4f:55:b1:10
          Signal              : 26%
          Radio type          : 802.11n
          Band                : 2.4 GHz
          Channel             : 8
          Bss Load:
              Connected Stations:        0
              Channel Utilization:       10 (3 %)
              Medium Available Capacity: 0 (0 us/s)
          QoS MSCS Supported    : 0
          QoS Map Supported     : 0
          Basic rates (Mbps) : 12 24
          Other rates (Mbps) : 18 36 48 54
    BSSID 6                 : 70:3a:0e:e8:10:32
          Signal              : 20%
          Radio type          : 802.11ac
          Band                : 5 GHz
          Channel             : 52
          QoS MSCS Supported    : 0
          QoS Map Supported     : 0
          Basic rates (Mbps) : 6 9 12 24
          Other rates (Mbps) : 18 36 48 54
    BSSID 7                 : 70:3a:0e:ea:7f:d2
          Signal              : 65%
          Radio type          : 802.11ac
          Band                : 5 GHz
          Channel             : 161
          QoS MSCS Supported    : 0
          QoS Map Supported     : 0
          Basic rates (Mbps) : 6 9 12 24
          Other rates (Mbps) : 18 36 48 54
    BSSID 8                 : 70:3a:0e:ea:7f:c0
          Signal              : 80%
          Radio type          : 802.11n
          Band                : 2.4 GHz
          Channel             : 6
          QoS MSCS Supported    : 0
          QoS Map Supported     : 0
          Basic rates (Mbps) : 5.5 6 9 12 24
          Other rates (Mbps) : 11 18 36 48 54

SSID 2 : AirFiber-Praba
    Network type            : Infrastructure
    Authentication          : WPA2-Personal
    Encryption              : CCMP
    BSSID 1                 : be:55:db:ab:6d:fe
          Signal              : 18%
          Radio type          : 802.11ax
          Band                : 2.4 GHz
          Channel             : 6
          QoS MSCS Supported    : 0
          QoS Map Supported     : 0
          Basic rates (Mbps) : 1 2 5.5 11
          Other rates (Mbps) : 6 9 12 18 24 36 48 54
"""

# ==================================================
# 2. Parsing and Feature Calculation Functions
#    (Corrected and Combined)
# ==================================================

# Lookup table to map Channel & Band to Frequency
FREQ_LOOKUP = {
    '2.4 GHz': {
        1: 2412, 2: 2417, 3: 2422, 4: 2427, 5: 2432, 6: 2437, 7: 2442, 8: 2447,
        9: 2452, 10: 2457, 11: 2462, 12: 2467, 13: 2472, 14: 2484
    },
    '5 GHz': {
        36: 5180, 40: 5200, 44: 5220, 48: 5240, 52: 5260, 56: 5280, 60: 5300,
        64: 5320, 100: 5500, 104: 5520, 108: 5540, 112: 5560, 116: 5580, 120: 5600,
        124: 5620, 128: 5640, 132: 5660, 136: 5680, 140: 5700, 144: 5720, 149: 5745,
        153: 5765, 157: 5785, 161: 5805, 165: 5825
    }
}

def parse_netsh_output(raw_text):
    networks = []
    lines = raw_text.splitlines()
    current_ssid = None
    bssid_info = {}

    for line in lines:
        line = line.strip()
        if not line:
            continue

        if line.startswith("SSID"):
            if current_ssid:
                networks.append(bssid_info)
            current_ssid = line.split(":", 1)[1].strip()
            bssid_info = {'SSID': current_ssid}

        elif line.startswith("BSSID"):
            if 'BSSID' in bssid_info:
                networks.append(bssid_info.copy())
            bssid_info = {'SSID': current_ssid, 'BSSID': line.split(":", 1)[1].strip()}

        elif ":" in line:
            key, value = line.split(":", 1)
            key = key.strip()
            value = value.strip()

            if key == "Signal":
                bssid_info['Signal_percent'] = int(re.search(r'(\d+)%', value).group(1))
            elif key == "Radio type":
                bssid_info['Radio_type'] = value
            elif key == "Band":
                bssid_info['Band'] = value
            elif key == "Channel":
                bssid_info['Channel'] = int(value)
            elif key == "Channel Utilization":
                bssid_info['Channel_Utilization_percent'] = int(re.search(r'(\d+) %', value).group(1))

    if 'BSSID' in bssid_info:
        networks.append(bssid_info)

    return networks

def calculate_derived_features(data_row, latitude, longitude):
    NOISE_FLOOR = -90.0
    RSSI_0 = -40.0
    PATH_LOSS_EXPONENT = 2.7
    BANDWIDTH_2_4GHZ = 20e6
    BANDWIDTH_5GHZ = 40e6

    rssi_dbm = 0.5 * data_row.get('Signal_percent', 0) - 100
    snr = rssi_dbm - NOISE_FLOOR
    if snr < 0: snr = 0

    if data_row.get('Band') == '2.4 GHz':
        B = BANDWIDTH_2_4GHZ
    elif data_row.get('Band') == '5 GHz':
        B = BANDWIDTH_5GHZ
    else:
        B = 20e6

    throughput_bps = B * math.log2(1 + 10**(snr / 10))
    throughput_mbps = throughput_bps / 1e6
    ap_load = data_row.get('Channel_Utilization_percent', 0) / 100.0

    latency = 1.0 / (snr + 1) * 100 + ap_load * 50 + random.uniform(0, 10)
    jitter = random.uniform(0.01, 0.1) * latency
    packet_loss = 0.5 * math.exp(-0.1 * snr) + ap_load * 0.1 + random.uniform(0, 0.05)
    packet_loss_percent = packet_loss * 100

    MIN_MAX = {
        'RSSI': [-100, -30], 'Throughput': [0, 500], 'SNR': [0, 70],
        'Latency': [20, 100], 'Jitter': [0, 10], 'Packet_Loss': [0, 50]
    }

    def normalize(value, min_val, max_val):
        if max_val - min_val == 0: return 0
        return (value - min_val) / (max_val - min_val)

    rssi_norm = normalize(rssi_dbm, MIN_MAX['RSSI'][0], MIN_MAX['RSSI'][1])
    throughput_norm = normalize(throughput_mbps, MIN_MAX['Throughput'][0], MIN_MAX['Throughput'][1])
    snr_norm = normalize(snr, MIN_MAX['SNR'][0], MIN_MAX['SNR'][1])

    latency_inv = 1 - normalize(latency, MIN_MAX['Latency'][0], MIN_MAX['Latency'][1])
    jitter_inv = 1 - normalize(jitter, MIN_MAX['Jitter'][0], MIN_MAX['Jitter'][1])
    packet_loss_inv = 1 - normalize(packet_loss_percent, MIN_MAX['Packet_Loss'][0], MIN_MAX['Packet_Loss'][1])

    composite_score = (rssi_norm + throughput_norm + snr_norm + latency_inv + jitter_inv + packet_loss_inv) / 6.0
    estimated_distance = 10**((RSSI_0 - rssi_dbm) / (10 * PATH_LOSS_EXPONENT))

    frequency = FREQ_LOOKUP.get(data_row.get('Band'), {}).get(data_row.get('Channel'), 0)

    if ap_load < 0.3:
        congestion_level = 0
    elif ap_load < 0.7:
        congestion_level = 1
    else:
        congestion_level = 2

    return {
        'BSSID': data_row.get('BSSID', 'N/A'),
        'RSSI': int(rssi_dbm),
        'Channel': data_row.get('Channel', 0),
        'SNR': round(snr),
        'Throughput': round(throughput_mbps, 2),
        'Latency': round(latency, 2),
        'Jitter': round(jitter, 2),
        'Packet_Loss': round(packet_loss_percent, 2),
        'Composite_Score': round(composite_score, 2),
        'Estimated_Distance': round(estimated_distance, 2),
        'AP_Load': round(ap_load, 2),
        'latitude': latitude,
        'longitude': longitude,
        'accuracy': 5.0,
        'totalSurveys': 1,
        'inconsistentLocation': 0,
        'Optimal_AP': 0,
        'Optimal_Rate': 0,
        'distance_moved': 0.0,
        'Mobility_Flag': False,
        'Recovered_From_Disconnect': False,
        'Avg_Neighbor_RSSI': -75.0,
        'Avg_Throughput_BSSID': round(throughput_mbps, 2),
        'Avg_PacketLoss_BSSID': round(packet_loss_percent, 2),
        'Fairness_Index': 1.0,
        'frequency': frequency,
        'Congestion_Level': congestion_level
    }

# ==================================================
# 3. Main execution to generate and predict
# ==================================================
if __name__ == "__main__":
    # --- Part A: Generate Live Sample ---
    print("Step 1: Parsing live Wi-Fi data...")
    parsed_networks = parse_netsh_output(NETSH_OUTPUT)

    current_latitude = 52.2530
    current_longitude = -2.8910
    live_samples = []

    for network in parsed_networks:
        sample = calculate_derived_features(network, current_latitude, current_longitude)
        live_samples.append(sample)

    print("Live sample generated successfully.")

    # --- Part B: Predict on Live Sample ---
    try:
        print("\nStep 2: Loading the trained model and label encoder...")
        model = joblib.load('optimized_model.pkl')
        le = joblib.load('label_encoder.pkl')
        print("Models loaded successfully.")
    except FileNotFoundError:
        print("\nError: Could not find model files.")
        print("Please ensure 'optimized_model.pkl' and 'label_encoder.pkl' are in the same directory.")
        exit()

    print("\nStep 3: Preparing data for prediction...")
    df_live = pd.DataFrame(live_samples)

    # Ensure the columns match the model's training features exactly
    feature_names = model.feature_names_in_
    X_live = df_live[feature_names]

    print("\nStep 4: Making predictions...")
    y_pred_indices = model.predict(X_live)
    y_pred_labels = le.inverse_transform(y_pred_indices)
    y_proba = model.predict_proba(X_live)

    print("\n==== Final Prediction Results ====")
    for i, sample in enumerate(live_samples):
        print(f"\n--- Analysis for BSSID: {sample['BSSID']} ---")

        top1_ap = y_pred_labels[i]
        top1_proba = np.max(y_proba[i])
        print(f"Top 1 Recommended AP: {top1_ap} (Probability: {top1_proba:.2f})")

        top5_indices = np.argsort(y_proba[i])[-5:][::-1] # Get top 5 indices, reversed

        top5_results = []
        for idx in top5_indices:
            ap_label = le.inverse_transform([idx])[0]
            probability = y_proba[i][idx]
            top5_results.append({'AP_Label': ap_label})

        print("Top 5 Recommendations:")
        pprint.pprint(top5_results)