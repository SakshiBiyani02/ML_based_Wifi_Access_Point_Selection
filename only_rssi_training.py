# -*- coding: utf-8 -*-
"""only_rssi_training

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yoFy3pDmK6TowHsRjs1dDuvy4c02ICBP
"""

# train_model_rssi_only.py

# ==================================================
# Imports
# ==================================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import (
    train_test_split, StratifiedKFold, cross_val_score, GridSearchCV
)
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    classification_report, accuracy_score,
    confusion_matrix, top_k_accuracy_score
)

# ==================================================
# 1. Load Dataset
# ==================================================
df = pd.read_csv("updated_project_dataset.csv")
print("Dataset shape:", df.shape)

# ==================================================
# 2. Drop Non-Numeric Columns
# ==================================================
non_numeric_cols = ['BSSID', 'recordDate', 'ssid', 'securityType', 'capabilities']
df = df.drop(columns=non_numeric_cols, errors='ignore')
print("Dropped columns:", non_numeric_cols)

# ==================================================
# 3. Define Features & Target (MODIFIED)
# ==================================================
# Only select the 'RSSI' column as the feature for training
X = df[['RSSI']]
y = df['Best_AP_Label']
print("Initial AP classes:", y.nunique())
print("Training on a single feature: 'RSSI'")

# ==================================================
# 4. Filter Classes with < 2 Samples
# ==================================================
counts = y.value_counts()
valid_classes = counts[counts > 1].index
df = df[df['Best_AP_Label'].isin(valid_classes)]

# Redefine X and y after filtering
X = df[['RSSI']]
y = df['Best_AP_Label']
print("After filtering shape:", df.shape)
print("Remaining AP classes:", y.nunique())

# Encode target labels
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# ==================================================
# 5. Train-Test Split
# ==================================================
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded,
    test_size=0.2,
    random_state=42,
    stratify=y_encoded
)

# ==================================================
# 6. Hyperparameter Tuning (GridSearchCV)
# ==================================================
param_grid = {
    'n_estimators': [100, 300, 500],
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5]
}

rf_base = RandomForestClassifier(random_state=42, n_jobs=-1)

grid_search = GridSearchCV(
    estimator=rf_base,
    param_grid=param_grid,
    scoring='accuracy',
    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
    n_jobs=-1,
    verbose=1
)

print("\n==== Starting Hyperparameter Tuning... ====")
grid_search.fit(X_train, y_train)

rf_best = grid_search.best_estimator_
print("\n==== Best Hyperparameters Found ====")
print(grid_search.best_params_)
print("Best CV Score:", grid_search.best_score_)

# ==================================================
# 7. Cross-Validation with Best Model
# ==================================================
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = cross_val_score(rf_best, X, y_encoded, cv=cv, scoring="accuracy", n_jobs=-1)

print("\n==== Cross-Validation Results (5-fold) ====")
print("Scores:", cv_scores)
print("Mean CV Accuracy:", np.mean(cv_scores))

# ==================================================
# 8. Train & Evaluate Best Model
# ==================================================
rf_best.fit(X_train, y_train)
y_train_pred = rf_best.predict(X_train)
y_test_pred = rf_best.predict(X_test)

# --- Training report
print("\n==== Training Metrics ====")
print("Accuracy:", accuracy_score(y_train, y_train_pred))
print(classification_report(
    y_train, y_train_pred,
    labels=np.unique(y_train),
    target_names=le.classes_[np.unique(y_train)]
))

# --- Testing report
print("\n==== Testing Metrics ====")
print("Top-1 Accuracy:", accuracy_score(y_test, y_test_pred))

y_test_proba = rf_best.predict_proba(X_test)
top5_acc = top_k_accuracy_score(
    y_test, y_test_proba, k=5,
    labels=range(len(le.classes_))
)
print("Top-5 Accuracy:", top5_acc)

print(classification_report(
    y_test, y_test_pred,
    labels=np.unique(y_test),
    target_names=le.classes_[np.unique(y_test)],
    zero_division=0
))

# ==================================================
# 9. Confusion Matrix Heatmap
# ==================================================
cm = confusion_matrix(y_test, y_test_pred, labels=np.unique(y_test))
plt.figure(figsize=(18, 14))
sns.heatmap(cm, annot=False, cmap="Blues",
            xticklabels=le.classes_[np.unique(y_test)],
            yticklabels=le.classes_[np.unique(y_test)])
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix Heatmap (Test Set) - RSSI-only Model")
plt.savefig('rssi_only_confusion_matrix_heatmap.png')
plt.clf()

# ==================================================
# 10. Feature Importance Plot
# ==================================================
feature_importances = pd.Series(rf_best.feature_importances_, index=X_train.columns)
sorted_features = feature_importances.sort_values(ascending=False)

plt.figure(figsize=(12, 8))
sns.barplot(x=sorted_features.values, y=sorted_features.index, palette='viridis')
plt.title('Feature Importances (RSSI-only Model)')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.tight_layout()
plt.savefig('rssi_only_feature_importance.png')
plt.clf()

# ==================================================
# 11. Correlation Matrix Heatmap
# ==================================================
correlation_matrix = X.corr()
plt.figure(figsize=(18, 16))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm')
plt.title('Correlation Matrix of RSSI-only Feature')
plt.tight_layout()
plt.savefig('rssi_only_correlation_matrix_heatmap.png')
plt.clf()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier

# ==================================================
# 1. Load Dataset & Preprocess
# ==================================================
df = pd.read_csv("updated_project_dataset.csv")
non_numeric_cols = ['BSSID', 'recordDate', 'ssid', 'securityType', 'capabilities']
df = df.drop(columns=non_numeric_cols, errors='ignore')

# Features and Target for RSSI-only model
X_rssi_only = df[['RSSI']]
y = df['Best_AP_Label']

# Filter classes with < 2 samples
counts = y.value_counts()
valid_classes = counts[counts > 1].index
df = df[df['Best_AP_Label'].isin(valid_classes)]

# Redefine features and target after filtering
X_rssi_only = df[['RSSI']]
y = df['Best_AP_Label']

# Encode target
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# Train-test split (required for model training)
X_rssi_only_train, _, _, _ = train_test_split(
    X_rssi_only, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
)

# ==================================================
# 2. Train the RSSI-only Model
# ==================================================
print("Training the RSSI-only Model...")
rf_rssi_only = RandomForestClassifier(
    n_estimators=300, max_depth=20, min_samples_split=5, random_state=42, n_jobs=-1
)
rf_rssi_only.fit(X_rssi_only_train, y_train)
print("Model trained successfully.")

# ==================================================
# 3. Define live samples (from your provided code)
# ==================================================
live_samples = [
    {
        'RSSI': -30
    },
    {
        'RSSI': -50
    },
    {
        'RSSI': -40
    },
    {
        'RSSI': -60
    },
    {
        'RSSI': -20
    }
]

# ==================================================
# 4. Predict for each sample
# ==================================================
print("\n==== RSSI-only Model Predictions ====")
for i, sample in enumerate(live_samples):
    live_sample_df_rssi_only = pd.DataFrame([sample])

    # Get Top-5 predictions from the RSSI-only model
    top5_proba_rssi = rf_rssi_only.predict_proba(live_sample_df_rssi_only)[0]
    top5_indices_rssi = np.argsort(top5_proba_rssi)[::-1][:5]
    top5_ap_rssi = le.inverse_transform(top5_indices_rssi)

    # Get Top-1 prediction
    top1_prediction_rssi = rf_rssi_only.predict(live_sample_df_rssi_only)[0]
    top1_ap_rssi = le.inverse_transform([top1_prediction_rssi])[0]

    # Print results
    print(f"\n--- Sample {i+1} (RSSI: {sample['RSSI']} dBm) ---")
    print(f"Predicted Top-1 AP: {top1_ap_rssi}")
    print(f"Top-5 Recommended APs: {top5_ap_rssi}")

# comparison_graphs.py

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report

# ==================================================
# 1. Manually Input Your Results Here
# ==================================================
# Replace the placeholder values (e.g., 0.XX) with your actual results.
# The Top-1 and Top-5 accuracy values come directly from your console output.
advanced_metrics = {
    'top1_acc': 0.81,
    'top5_acc': 0.97
}

rssi_metrics = {
    'top1_acc': 0.0267, # <-- REPLACE THIS WITH YOUR RSSI-ONLY MODEL'S TOP-1 ACCURACY
    'top5_acc': 0.1708  # <-- REPLACE THIS WITH YOUR RSSI-ONLY MODEL'S TOP-5 ACCURACY
}

# You can also manually create a dictionary from the classification reports for a more detailed comparison
# Example:
# advanced_report = {
#     'AP12': {'precision': 0.85, 'recall': 0.90, 'f1-score': 0.87, 'support': 26},
#     'AP23': {'precision': 0.78, 'recall': 0.82, 'f1-score': 0.80, 'support': 60},
#     ...
# }

# ==================================================
# 2. Accuracy Comparison Bar Chart
# ==================================================
def plot_accuracy_comparison(metrics_advanced, metrics_rssi):
    labels = ['Top-1 Accuracy', 'Top-5 Accuracy']
    advanced_scores = [metrics_advanced['top1_acc'], metrics_advanced['top5_acc']]
    rssi_scores = [metrics_rssi['top1_acc'], metrics_rssi['top5_acc']]

    x = np.arange(len(labels))
    width = 0.35

    fig, ax = plt.subplots(figsize=(10, 6))
    rects1 = ax.bar(x - width/2, advanced_scores, width, label='Advanced Model', color='darkblue')
    rects2 = ax.bar(x + width/2, rssi_scores, width, label='RSSI-only Model', color='skyblue')

    ax.set_ylabel('Accuracy')
    ax.set_title('Top-1 and Top-5 Accuracy Comparison')
    ax.set_xticks(x)
    ax.set_xticklabels(labels)
    ax.legend()
    ax.set_ylim(0, 1.1)

    # Add labels on the bars
    for rect in rects1 + rects2:
        height = rect.get_height()
        ax.annotate(f'{height:.2f}',
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3),
                    textcoords="offset points",
                    ha='center', va='bottom')

    plt.tight_layout()
    plt.savefig('accuracy_comparison.png')
    plt.show()
    print("Generated 'accuracy_comparison.png'")

# ==================================================
# 3. Top-K Accuracy Line Plot (More Detailed)
# ==================================================
# This requires a more detailed calculation that you can do within your notebooks.
# For simplicity, we can plot a line based on the two data points we have.
def plot_top_k_comparison(metrics_advanced, metrics_rssi):
    k_values = [1, 5]
    advanced_accuracies = [metrics_advanced['top1_acc'], metrics_advanced['top5_acc']]
    rssi_accuracies = [metrics_rssi['top1_acc'], metrics_rssi['top5_acc']]

    plt.figure(figsize=(10, 6))
    plt.plot(k_values, advanced_accuracies, marker='o', linestyle='-', label='Advanced Model', color='darkblue')
    plt.plot(k_values, rssi_accuracies, marker='o', linestyle='--', label='RSSI-only Model', color='skyblue')

    plt.title('Top-K Accuracy Comparison')
    plt.xlabel('k (Number of Top Recommendations)')
    plt.ylabel('Accuracy')
    plt.xticks(k_values, ['1', '5'])
    plt.ylim(0, 1.1)
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig('top_k_comparison.png')
    plt.show()
    print("Generated 'top_k_comparison.png'")


# ==================================================
# 4. Run the Plots
# ==================================================
if __name__ == "__main__":
    plot_accuracy_comparison(advanced_metrics, rssi_metrics)
    plot_top_k_comparison(advanced_metrics, rssi_metrics)

# Add this import at the top
from sklearn.metrics import log_loss

# Add this to the '==== Testing Metrics ====' section
# You must calculate this using the test set probabilities
# For log_loss, you need to use all predicted probabilities, not just the top-5
logloss = log_loss(y_test, y_test_proba, labels=range(len(le.classes_)))
print(f"Log-Loss: {logloss:.4f}")

# Add this import at the top
from sklearn.metrics import matthews_corrcoef

# Add this to the '==== Testing Metrics ====' section
# You must use the predicted class labels, not the probabilities
mcc = matthews_corrcoef(y_test, y_test_pred)
print(f"Matthews Correlation Coefficient (MCC): {mcc:.4f}")